addpath(genpath('../toolbox/'));

% Load both features and training images
load train_feats;
load train_imgs;

% %% --browse through the images, and show the feature visualization beside
% %  -- You should explore the features for the positive and negative
% %  examples and understand how they resemble the original image.
% for i=1:2
%     clf();
%     
%     subplot(121);
%     imshow(imgs{i}); % image itself
%     
%     subplot(122);
%     im( hogDraw(feats{i}) ); colormap gray;
%     axis off; colorbar off;
%     
%     pause;  % wait for keydo that then,??
% end

%% -- Generate feature vectors (so each one is a row of X)
fprintf('Generating feature vectors..\n');
D = numel(feats{1});  % feature dimensionality
X = zeros([length(imgs) D]);

for i=1:length(imgs)
    X(i,:) = feats{i}(:);  % convert to a vector of D dimensions
end


% -- Example: split half and half into train/test
% fprintf('Splitting into train/test..\n');
% % NOTE: you should do this randomly! and k-fold!
% Tr.idxs = 1:2:size(X,1);
% Tr.X = X(Tr.idxs,:);
% Tr.y = labels(Tr.idxs);
% 
% Te.idxs = 2:2:size(X,1);
% Te.X = X(Te.idxs,:);
% Te.y = labels(Te.idxs);
% 
% [Tr.X,Tr.y,Te.X,Te.y]=splitFair(X,labels,0.8,1) ; 
% 
% %% Train an SVM Model
% % gamma = mean(pdist(Tr.X))  
% % param{1} = sprintf('%s %d %s %d','-t 2 -c', 100, '-g',gamma) ; 
% fprintf('Training SVM Model.. Begin\n');
% 
% model1 = svmtrain(Tr.y, Tr.X , '-t 2 -c 100 ');
% [predicted_label, accuracy, decision_values1] = svmpredict(Te.y, Te.X, model1 );
% 
% %%
% 
% 
% [Tr.X,Tr.y,Te.X,Te.y]=splitAndSubSample(X,labels,0.8,8,1) ; 
% 
% %% Train an SVM Model
% % gamma = mean(pdist(Tr.X))  
% % param{1} = sprintf('%s %d %s %d','-t 2 -c', 100, '-g',gamma) ; 
% fprintf('Training SVM Model.. Begin\n');
% 
% model2 = svmtrain(Tr.y, Tr.X , '-t 2 -c 1 -g 0.1 ');
% [predicted_label, accuracy, decision_values2] = svmpredict(Te.y, Te.X, model2 );
% 
% [U,S,V]=svd(X) ; 
% X = U*S ; 
% X=X(:,1:3000) ; 
% [Tr.X,Tr.y,Te.X,Te.y]=splitAndSubSample(X,labels,0.8,8,1) ; 
% 
% model3 = svmtrain(Tr.y, Tr.X , '-t 2 -c 1 -g 0.1 ');
% [predicted_label, accuracy, decision_values3] = svmpredict(Te.y, Te.X, model3 );
% 
% 
% 
% %% See prediction performance
% fprintf('Plotting performance..\n');
% % let's also see how random predicition does
% randPred = rand(size(Te.y));
% 
% % and plot all together, and get the performance of each
% methodNames = {'SVM complete Data', 'SVM SubSampledData'}; % this is to show it in the legend
% avgTPRList = evaluateMultipleMethods( Te.y > 0, [decision_values3,decision_values2], true, methodNames );
% 
% % now you can see that the performance of each method
% % is in avgTPRList. You can see that random is doing very bad.
% avgTPRList

%%
% [U,S,V]=svd(X) ; 
% X = U*S ; 
% X=X(:,1:3000) ; 
% rangeGamma = logspace(-2,2,100);
% C = 1:1:100 ;
kernel =2 ; 

 rangeGamma =logspace(-2,-1,20) ;
<<<<<<< HEAD
  
 rangeC =  
rangeC= 50 ; 
=======
 rangeC = 19:1:20; 
>>>>>>> 0ab9914371cba73fb884cbc911d9a5f5fb4a086f
K = 5 ; 



[ scores, scoreBest, GammaBest, CBest ] = findBestHyperPArametersSVM( X, labels, rangeGamma, rangeC,kernel,  K  )








